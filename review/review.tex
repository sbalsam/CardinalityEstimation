\documentclass[twocolumn, 9pt]{extarticle}
\usepackage[top=1in, bottom=1.25in, left=0.8in, right=0.8in]{geometry}
\usepackage{flushend} 
\usepackage{algorithm}
\usepackage{algpseudocode}

% -- Single column version
%\documentclass[11pt]{article}
%\usepackage[top=1in, bottom=1in, left=1.5in, right=1.5in]{geometry}

\usepackage{lmodern}		% nice font
\usepackage[final]{microtype}	% better hyphenation

% Optional packages for enhanced functionality
\usepackage{graphicx} % For including images


\title{Cardinality Estimation of Distinct Items - An Overview}
\author{Sebastian Balsam}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
In this paper I want to give an overview of different solutions
for the Count Distinct Problem...
\end{abstract}

\section{Introduction}

The cardinality estimation problem or count-distinct problem is about finding
the number of distinct elements in a data stream with repeated elements. These
elements could be url's, unique users, sensor data or IP addresses passing
through a data stream. A simple solution would be to use a list and add an item
to this list each time we encounter an unseen item. With this approach we can
give an exact answer to the query, how many distinct elements have been
seen. Unfortunately if millions of distinct elements are present in a data
stream, with this approach, we will soon hit storage boundaries and the
performance will detoriate. 

As we often do not need an exact answer, streaming algorithms have been 
developed, that give an approximation that is mostly good enough and bound to 
a fixed storage size. There different approaches to solve this problem.
Sampling techniques are used as an estimate by sampling a subset of
items in the set and use the subset as estimator, e.g the CVM algorithm
by Chakraborty et al. \cite{cvm2023}. While sampling methods generally
give a good estimate, they can fail in certain situations. If rare elements
are not in the sampled set, they would not be counted and we would 
underestimate. For database optimization machine
learning techniques have been used recently for cardinality estimation
\cite{Liu2015, Woltmann2019, Schwabe2024}. 

The technique I want to focus in this paper are sketch techniques that
implement a certain data structure and an accompanying algorithm to store
information efficient for cardinality estimation.

\section{Probabilistic Counting with Stochastic Averaging}

- Overview of PCSA
- Problem description

\subsection{Flajolet \& Martin}

In the 80's streaming problems were not the problems, we have today, as there
simply were not that many data streams. But databases existed and began to
grow in size. As table sizes grew, evaluation strategies became important,
how to handle such big data tables for join operations. Query optimizers
were developed that could find the optimal strategy.

In this context Flajolet and Martin developed a method to estimate
the number of distinct items in a set in a space efficient way\cite{fm85}.

\begin{algorithm}
\caption{The Flajolet Martin algorithm.}
	\label{alg:fm85}
	\begin{algorithmic}[1]
\Require $n \geq 0$
\Ensure $y = x^n$
\State $y \gets 1$
\State $X \gets x$
\State $N \gets n$
\While{$N \neq 0$}
\If{$N$ is even}
    \State $X \gets X \times X$
    \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
\ElsIf{$N$ is odd}
    \State $y \gets y \times X$
    \State $N \gets N - 1$
\EndIf
\EndWhile

\end{algorithmic}
\end{algorithm}

% check https://book.moa.cms.waikato.ac.nz/chapter_4.html/
- Compressed (Scheuermann)

\section{HyperLogLog}

\section{Conclusion}

\bibliographystyle{apalike}
\bibliography{bibfile}

\end{document}
